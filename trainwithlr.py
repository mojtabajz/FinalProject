# -*- coding: utf-8 -*-
"""trainwithLR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iuyEYqu45dA34I9fy_5C4pghIqbxvGES
"""

# Load Persian stopwords
print("Loading stopwords...")
with open('stopwords.txt', 'r', encoding='utf-8') as f:
    stopwords = set(f.read().splitlines())

import pandas as pd
import re
from tqdm import tqdm

# Load and prepare data
print("Loading data...")
df = pd.read_csv('/mnt/HDDa/tayebe/ghadir/stress detection/almas/data_4030611.csv')
df = df[['text', 'label']]

# Load Persian stopwords
print("Loading stopwords...")
with open('stopwords.txt', 'r', encoding='utf-8') as f:
    stopwords = set(f.read().splitlines())

# Preprocessing functions
def preprocess_text(text):
    text = re.sub(r'@\w+', '', text)  # Remove mentions
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Remove URLs
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)  # Remove non-Persian characters
    return text

def remove_stopwords(text, stopwords):
    return ' '.join(word for word in text.split() if word not in stopwords)

# Apply preprocessing to the text column with progress bar
print("Preprocessing training data...")
df['text'] = df['text'].fillna('')

# Apply both preprocessing functions using tqdm
df['text'] = tqdm(df['text'].apply(lambda x: remove_stopwords(preprocess_text(x), stopwords)))

X_train = df.text.tolist()
y_train = df.label.tolist()

# توزیع برچسب‌ها (کلاس‌ها)
label_counts = df['label'].value_counts()
print("Label Distribution:")
print(label_counts)

# رسم نمودار توزیع برچسب‌ها
import matplotlib.pyplot as plt
label_counts.plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('Distribution of Stress and No Stress')
plt.xlabel('Label (0 = No Stress, 1 = Stress)')
plt.ylabel('Count')
plt.show()

# ایجاد ستون طول جملات (تعداد کلمات در هر جمله)
df['text_length'] = df['text'].apply(lambda x: len(x.split()))

# رسم نمودار طول جملات در فضای لگاریتمی
plt.figure(figsize=(10, 6))

# جملات بدون استرس
plt.hist(df[df['label'] == 0]['text_length'], bins=30, alpha=0.5, label='No Stress', color='skyblue', log=True)

# جملات با استرس
plt.hist(df[df['label'] == 1]['text_length'], bins=30, alpha=0.5, label='Stress', color='salmon', log=True)

plt.title('Sentence Length Distribution by Label (Log Scale)')
plt.xlabel('Sentence Length')
plt.ylabel('Frequency (Log Scale)')
plt.legend()
plt.show()

# تفکیک توییت‌های استرس‌زا و بدون استرس
stress_data = df[df['label'] == 1]['text'].tolist()
non_stress_data = df[df['label'] == 0]['text'].tolist()

# تقسیم طول جملات به دو گروه
stress_lengths = df[df['label'] == 1]['text_length']
no_stress_lengths = df[df['label'] == 0]['text_length']

# محاسبه میانگین طول جملات برای هر گروه
mean_stress_length = stress_lengths.mean()
mean_no_stress_length = no_stress_lengths.mean()

print(f"Mean sentence length for stress group: {mean_stress_length}")
print(f"Mean sentence length for no-stress group: {mean_no_stress_length}")

from scipy.stats import mannwhitneyu

# انجام آزمون من-ویتنی
u_stat, p_value = mannwhitneyu(stress_lengths, no_stress_lengths)

print(f"U-statistic: {u_stat}")
print(f"P-value: {p_value}")

import matplotlib.pyplot as plt
import seaborn as sns

# ایجاد نمودار جعبه‌ای برای طول جملات در هر گروه
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['label'], y=df['text_length'])
plt.xticks([0, 1], ['No Stress', 'Stress'])
plt.title('Sentence Length Distribution by Stress Group')
plt.xlabel('Group')
plt.ylabel('Sentence Length')
plt.show()

# محاسبه میانه و مد طول جملات برای هر گروه
median_stress_length = stress_lengths.median()
median_no_stress_length = no_stress_lengths.median()

mode_stress_length = stress_lengths.mode()[0]
mode_no_stress_length = no_stress_lengths.mode()[0]

print(f"Median sentence length for stress group: {median_stress_length}")
print(f"Median sentence length for no-stress group: {median_no_stress_length}")
print(f"Mode sentence length for stress group: {mode_stress_length}")
print(f"Mode sentence length for no-stress group: {mode_no_stress_length}")

from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 1. استخراج کلمات کلیدی برای توییت‌های استرس‌زا
vectorizer = TfidfVectorizer()
X_stressed = vectorizer.fit_transform(stress_data)
feature_names = vectorizer.get_feature_names_out()
stressed_keyword_importance = dict(zip(feature_names, X_stressed.sum(axis=0).A1))

# نرمال‌سازی امتیاز کلمات کلیدی برای توییت‌های استرس‌زا
max_stressed_importance = max(stressed_keyword_importance.values())
min_stressed_importance = min(stressed_keyword_importance.values())

for keyword, importance in stressed_keyword_importance.items():
    stressed_keyword_importance[keyword] = (importance - min_stressed_importance) / (max_stressed_importance - min_stressed_importance)


# 2. استخراج کلمات کلیدی برای توییت‌های بدون استرس
X_non_stressed = vectorizer.fit_transform(non_stress_data)
non_stressed_keyword_importance = dict(zip(feature_names, X_non_stressed.sum(axis=0).A1))

# نرمال‌سازی امتیاز کلمات کلیدی برای توییت‌های بدون استرس
max_non_stressed_importance = max(non_stressed_keyword_importance.values())
min_non_stressed_importance = min(non_stressed_keyword_importance.values())

for keyword, importance in non_stressed_keyword_importance.items():
    non_stressed_keyword_importance[keyword] = (importance - min_non_stressed_importance) / (max_non_stressed_importance - min_non_stressed_importance)

x = {keyword:stressed_keyword_importance[keyword] - non_stressed_keyword_importance.get(keyword,0) for keyword in stressed_keyword_importance}
# مرتب‌سازی کلمات بر اساس اهمیت
sorted_keywords = sorted(x.items(), key=lambda item: item[1], reverse=True)

# مرتب‌سازی کلمات بر اساس اهمیت
sorted_keywords = sorted(x.items(), key=lambda item: item[1], reverse=True)

# انتخاب ۲۰۰ کلمه با بیشترین امتیاز
top_200_keywords = sorted_keywords[:200]

import arabic_reshaper
from bidi.algorithm import get_display
top_200_keywords = {get_display(arabic_reshaper.reshape(k)):v for k,v in top_200_keywords}

top_200_keywords = {k:v for k,v in top_200_keywords}

pd.DataFrame(top_200_keywords).to_excel('non_stressed_kw.xlsx')

df = pd.read_excel("stressed_kw.xlsx")

x=df[[0,  1]].set_index(0).to_dict()[1]

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
from PIL import Image
from wordcloud import ImageColorGenerator

# ایجاد ابر کلمات
wc = WordCloud(
    font_path = "/home/ubunto/Downloads/Fonts/reg.ttf",
    width=5000,  # عرض تصویر
    height=5000,  # ارتفاع تصویر
    max_words=200,  # حداکثر تعداد کلمات
    background_color="white",  # رنگ پس‌زمینه
)

# تبدیل دیکشنری کلمات و امتیازها به فرمت قابل قبول برای WordCloud
# دیکشنری باید کلمات به همراه فراوانی آنها به‌صورت {کلمه: امتیاز} باشد
wc.generate_from_frequencies(top_200_keywords)

# نمایش تصویر
plt.figure(figsize=(5000 / 72, 5000 / 72), dpi=72)
plt.imshow(wc, interpolation="bilinear")
plt.axis('off')
plt.show()

# ذخیره تصویر نهایی
wc.to_file('wordcloud_example.png')

# 3. رسم ابرکلمات برای هر دسته



# رسم ابرکلمات برای توییت‌های بدون استرس
non_stressed_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(non_stressed_keyword_importance)
plt.figure(figsize=(10, 5))
plt.imshow(non_stressed_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Non-Stressed Tweets Keywords Cloud")
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# توییت‌های استرس‌زا و بدون استرس (برای مثال)
stressed_tweets = ['...']  # لیست توییت‌های استرس‌زا
non_stressed_tweets = ['...']  # لیست توییت‌های بدون استرس

# 1. استخراج کلمات کلیدی برای توییت‌های استرس‌زا
vectorizer = TfidfVectorizer()
X_stressed = vectorizer.fit_transform(stressed_tweets)
feature_names = vectorizer.get_feature_names_out()
stressed_keyword_importance = dict(zip(feature_names, X_stressed.sum(axis=0).A1))

# نرمال‌سازی امتیاز کلمات کلیدی برای توییت‌های استرس‌زا
max_stressed_importance = max(stressed_keyword_importance.values())
min_stressed_importance = min(stressed_keyword_importance.values())

for keyword, importance in stressed_keyword_importance.items():
    stressed_keyword_importance[keyword] = (importance - min_stressed_importance) / (max_stressed_importance - min_stressed_importance)

# 2. استخراج کلمات کلیدی برای توییت‌های بدون استرس
X_non_stressed = vectorizer.fit_transform(non_stressed_tweets)
non_stressed_keyword_importance = dict(zip(feature_names, X_non_stressed.sum(axis=0).A1))

# نرمال‌سازی امتیاز کلمات کلیدی برای توییت‌های بدون استرس
max_non_stressed_importance = max(non_stressed_keyword_importance.values())
min_non_stressed_importance = min(non_stressed_keyword_importance.values())

for keyword, importance in non_stressed_keyword_importance.items():
    non_stressed_keyword_importance[keyword] = (importance - min_non_stressed_importance) / (max_non_stressed_importance - min_non_stressed_importance)

# 3. رسم ابرکلمات برای هر دسته

# رسم ابرکلمات برای توییت‌های استرس‌زا
stressed_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(stressed_keyword_importance)
plt.figure(figsize=(10, 5))
plt.imshow(stressed_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Stressed Tweets Keywords Cloud")
plt.show()

# رسم ابرکلمات برای توییت‌های بدون استرس
non_stressed_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(non_stressed_keyword_importance)
plt.figure(figsize=(10, 5))
plt.imshow(non_stressed_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Non-Stressed Tweets Keywords Cloud")
plt.show()

# شناسایی کلمات پر تکرار در هر دسته
common_words = set(stress_word_counts[stress_word_counts > threshold].index) & set(no_stress_word_counts[no_stress_word_counts > threshold].index)

# حذف کلمات مشترک از هر دو دسته
filtered_stress_word_counts = stress_word_counts.drop(common_words)
filtered_no_stress_word_counts = no_stress_word_counts.drop(common_words)







import pandas as pd
from tqdm import tqdm
# Load the new test dataset
print("Loading and preprocessing test data...")
test_df = pd.read_excel('../test/labeled_data.xlsx')


# Define a function to calculate majority vote
def majority_vote(row):
    votes = [row['Najme'], row['Zeinab'], row['Vafayi']]
    res = -1
    if votes.count(0) > 2 : res = 0
    if votes.count(1) > 2: res = 1
    if row['Tayebe'] in [0,1] and row['Tayebe'] != res:
        return -1
    return res

# Calculate the majority vote for each row
print("Calculating majority vote...")
test_df['majority_vote'] = tqdm(test_df.apply(majority_vote, axis=1))
test_df = test_df[test_df['majority_vote'] != -1]
test_df['label'] = test_df['majority_vote']

d = pd.read_excel('../test/Untitled 1.xlsx')
test_df = pd.concat([test_df, d])

# Preprocessing the test data
print("Preprocessing test data...")
test_df['otext'] = test_df['text']
test_df['text'] = test_df['text'].fillna('')
test_df['text'] = tqdm(test_df['text'].apply(preprocess_text))
test_df['text'] = tqdm(test_df['text'].apply(lambda x: remove_stopwords(x, stopwords)))



X_test  = test_df.text.tolist()
y_test  = test_df.label.tolist()

test_df

df = df[~df.text.isin(test_df.text.tolist())]
X_train = df.text.tolist()
y_train = df.label.tolist()

len(df)

from sklearn.feature_extraction.text import TfidfVectorizer
# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=20000)  # You can adjust the number of features

# Fit and transform the training data
X_train_tfidf = tfidf.fit_transform(X_train)

# Transform the test data
X_test_tfidf = tfidf.transform(X_test)

def filter_words(word):
    # Regex to match words with Persian characters and spaces only
    # Ensures that digits and non-Persian characters are excluded
    # [^\d] ensures no digits, [\u0600-\u06FF\s] allows Persian characters and spaces
    # {3,} ensures that the word has at least 3 characters
    pattern = re.compile(r'^[\u0600-\u06FF\s]{3,}$', re.UNICODE)
    return bool(pattern.match(word)) and not any(char.isdigit() for char in word)

# Extract feature words
feature_words = tfidf.get_feature_names_out()


# Convert feature words to DataFrame
feature_words_df = pd.DataFrame(feature_words, columns=['Feature Words'])

# Apply filtering function
filtered_feature_words_df = feature_words_df[feature_words_df['Feature Words'].apply(filter_words)]

# Save to CSV for manual editing
#filtered_feature_words_df.to_excel('filtered_feature_words.xlsx')

filtered_feature_words_df

# After editing, reload the CSV
edited_feature_words_df = pd.read_excel('filtered_feature_words.xlsx')

edited_feature_words_df = edited_feature_words_df[~edited_feature_words_df['Feature Words'].isin(stopwords)]
# Convert back to a list
edited_feature_words = edited_feature_words_df['Feature Words'].tolist()

# Initialize a new TF-IDF Vectorizer with edited vocabulary
tfidf_edited = TfidfVectorizer(vocabulary=edited_feature_words)

# Fit and transform the training data
X_train_tfidf_edited = tfidf_edited.fit_transform(X_train)

from sklearn.linear_model import LogisticRegression

# Logistic Regression Classifier with edited features
classifier_edited = LogisticRegression()
classifier_edited.fit(X_train_tfidf_edited, y_train)

# Transform the test data
X_test_tfidf_edited = tfidf_edited.transform(X_test)

# Predictions with edited features
y_pred_edited = classifier_edited.predict(X_test_tfidf_edited)

# Evaluation
print(classification_report(y_test, y_pred_edited))

import fasttext

# Convert the dataframe into FastText format
def to_fasttext_format(df, text_column, label_column):
    lines = []
    for _, row in df.iterrows():
        label = f"__label__{row[label_column]}"
        text = row[text_column]
        lines.append(f"{label} {text}")
    return lines

fasttext_data = to_fasttext_format(df, 'text', 'label')

# Save the formatted data to a file for FastText
with open('training_data.txt', 'w') as f:
    for line in fasttext_data:
        f.write(line + '\n')
# Train the FastText model
model = fasttext.train_supervised('training_data.txt', epoch=100, lr=0.1, wordNgrams=4, verbose=2)

# Define a function to clean text (remove newlines, extra spaces, etc.)
def clean_text(text):
    # Remove newline characters and trim leading/trailing spaces
    return text.replace('\n', ' ').strip()

# Apply the cleaning function to the text column
test_df['text_cleaned'] = test_df['text'].apply(clean_text)
def get_fasttext_prediction(text, model):
    labels, probabilities = model.predict(text)
    # If you want only the most likely label
    return int(labels[0][-1]), probabilities[0]

# Apply the prediction function to each row and add results to new columns
y_pred2, y_prob = zip(*test_df['text_cleaned'].apply(lambda x: get_fasttext_prediction(x, model)))

# Evaluation
print(classification_report(y_test, y_pred2))

import numpy as np
import matplotlib.pyplot as plt
import warnings
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Bidirectional, LSTM, BatchNormalization
from tensorflow.keras.models import Model
from transformers import TFBertModel, BertTokenizerFast, BertConfig
import pandas as pd
import re

# DataGenerator class
class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self, data, batch_size, tokenizer, max_length, shuffle=True):
        self.data = data
        self.batch_size = batch_size
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.shuffle = shuffle
        self.indexes = np.arange(len(self.data))
        self.on_epoch_end()

    def __len__(self):
    # Return the total number of batches
        return int(np.ceil(len(self.data) / self.batch_size))


    def __getitem__(self, index):
        # Generate one batch of data
        start_idx = index * self.batch_size
        end_idx = min((index + 1) * self.batch_size, len(self.data))  # Ensure we don't go out of bounds
        indexes = self.indexes[start_idx:end_idx]
        batch_data = [self.data.iloc[k] for k in indexes]

        # Tokenize the batch
        texts = [item['text'] for item in batch_data]
        labels = np.array([item['label'] for item in batch_data])

        tokens = self.tokenizer(
            text=texts,
            add_special_tokens=True,
            return_tensors='tf',
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_token_type_ids=False,
            return_attention_mask=True
        )

        return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}, labels

    def on_epoch_end(self):
        # Updates indexes after each epoch
        if self.shuffle:
            np.random.shuffle(self.indexes)

import numpy as np
import pandas as pd
import re
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from transformers import TFBertModel, BertTokenizerFast, BertConfig
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# BERT Model Initialization
model_name = 'HooshvareLab/bert-fa-base-uncased'
config = BertConfig.from_pretrained(model_name, output_hidden_states=False)
tokenizer = BertTokenizerFast.from_pretrained(model_name, config=config)

batch_size = 64
max_length = 60

# Instantiate DataGenerators
train_generator = DataGenerator(train_data, batch_size, tokenizer, max_length)
valid_generator = DataGenerator(test_data, batch_size, tokenizer, max_length)

# Load the previously saved model with frozen BERT layers
model = load_model('../frozen_model.h5', custom_objects={'TFBertModel': TFBertModel})

# Unfreeze the BERT layers
bert_layer_name = 'tf_bert_model'  # Ensure this is the correct layer name
bert_layer = model.get_layer(bert_layer_name)
bert_layer.trainable = True

# Recompile the model to apply changes
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8, clipnorm=1.0)
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=optimizer,
    metrics=['accuracy']
)

# Define EarlyStopping and ModelCheckpoint
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

checkpoint = ModelCheckpoint('fine_tuned_checkpoint.h5', save_best_only=True, save_weights_only=True)

# Continue Training with Unfrozen BERT Layers
history_fine_tune = model.fit(
    train_generator,
    epochs=5,
    validation_data=test_generator,
    callbacks=[early_stopping, checkpoint]
)

# Save the Fine-Tuned Model
model.save('../fine_tuned_model.h5')

# Plot Training History
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history_fine_tune.history['accuracy'], label='Train Accuracy (Fine-tuning)')
plt.plot(history_fine_tune.history['val_accuracy'], label='Validation Accuracy (Fine-tuning)')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_fine_tune.history['loss'], label='Train Loss (Fine-tuning)')
plt.plot(history_fine_tune.history['val_loss'], label='Validation Loss (Fine-tuning)')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import re
import tensorflow as tf
from tensorflow.keras.models import load_model
from transformers import BertTokenizerFast, TFBertModel

# Load the fine-tuned model
model = load_model('../fine_tuned_model.h5', custom_objects={'TFBertModel': TFBertModel})

test_df['otext'] = test_df['text']
test_df['text'] = test_df['ctext']

batch_size = 64
max_length = 60

model_name = 'HooshvareLab/bert-fa-base-uncased'
tokenizer = BertTokenizerFast.from_pretrained(model_name)

# Tokenize the test data using the tokenizer
test_generator = DataGenerator(test_df, batch_size, tokenizer, max_length, shuffle=False)

# Get predictions from the model
predictions = model.predict(test_generator)

# Convert predictions to class labels
predicted_classes = np.argmax(predictions, axis=1)

# Extract the majority vote labels
true_labels = test_df['label'].values

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Calculate accuracy
accuracy = accuracy_score(true_labels, predicted_classes)
print(f'Accuracy: {accuracy:.4f}')

# Precision, Recall, F1-Score
precision = precision_score(true_labels, predicted_classes, average='binary')
recall = recall_score(true_labels, predicted_classes, average='binary')
f1 = f1_score(true_labels, predicted_classes, average='binary')

print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-Score: {f1:.4f}')

# Confusion Matrix
conf_matrix = confusion_matrix(true_labels, predicted_classes)
print('Confusion Matrix:')
print(conf_matrix)

# Detailed Classification Report
report = classification_report(true_labels, predicted_classes, target_names=['Class 0', 'Class 1'])
print('Classification Report:')
print(report)



x = test_df[['text', 'label']]

x['FastText'] = y_pred2
x['Bert'] = predicted_classes
x['Bert2'] = predicted_classes

x['svm'] = y_predsvm
x['LR'] = y_pred_edited

x['ensemble'] = x.apply(lambda row: 1 if row[['LR', 'FastText', 'svm', 'Bert', 'Bert2']].tolist().count(1) >= 3 else 0, axis=1)

# Detailed Classification Report
report = classification_report(true_labels, x['ensemble'].tolist(), target_names=['Class 0', 'Class 1'])
print('Classification Report:')
print(report)

x.to_excel('xxxxxxxxxxxxxxxxxxxxx.xlsx')

# Detailed Classification Report
report = classification_report(true_labels, x['ensemble'].tolist(), target_names=['Class 0', 'Class 1'])
print('Classification Report:')
print(report)

x[(x['ensemble'] == x['Bert'])  & (x['Bert'] != x['label']) & (x['label'] == 1)].to_excel('11111111111111111.xlsx')

import lightgbm as lgb

clf = lgb.LGBMClassifier(random_state=42)
clf.fit(X_train_tfidf_edited, y_train)

# Predictions with edited features
y_predlgb = clf.predict(X_test_tfidf_edited)

# Evaluation
print(classification_report(y_test, y_predlgb))

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()
clf.fit(X_train_tfidf_edited, y_train)

# Predictions with edited features
y_prednb = clf.predict(X_test_tfidf_edited)

# Evaluation
print(classification_report(y_test, y_prednb))

from sklearn.svm import SVC, LinearSVC

# Linear SVM (fast and efficient for large datasets)
clf = LinearSVC(random_state=42)
clf.fit(X_train_tfidf_edited, y_train)

# Predictions with edited features
y_predsvm = clf.predict(X_test_tfidf_edited)

# Evaluation
print(classification_report(y_test, y_predsvm))

import xgboost as xgb

clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
clf.fit(X_train_tfidf_edited, y_train)

# Predictions with edited features
y_predxgb = clf.predict(X_test_tfidf_edited)

# Evaluation
print(classification_report(y_test, y_predxgb))

from sklearn.neighbors import KNeighborsClassifier

clf = KNeighborsClassifier(n_neighbors=5)
clf.fit(X_train_tfidf_edited, y_train)


y_predknn = clf.predict(X_test_tfidf_edited)

# Evaluation
print(classification_report(y_test, y_predknn))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sum the TF-IDF scores across all documents in the test set
tfidf_scores = np.sum(X_test_tfidf_edited, axis=0).A1  # A1 flattens the sparse matrix to an array
tfidf_words = tfidf_edited.get_feature_names_out()

# Create a DataFrame with words and their corresponding TF-IDF scores
tfidf_df = pd.DataFrame({'Word': tfidf_words, 'TF-IDF Score': tfidf_scores})

# Sort the DataFrame by TF-IDF score in descending order
tfidf_df = tfidf_df.sort_values(by='TF-IDF Score', ascending=False)

# Display the top 20 words with the highest TF-IDF scores
top_words = tfidf_df.head(20)

# Plot the most important words
plt.figure(figsize=(10, 6))
sns.barplot(x='TF-IDF Score', y='Word', data=top_words)
plt.title('Top 20 Words by TF-IDF Score in Test Data')
plt.show()

tfidf_df.iloc[:50,:]

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X_test_tfidf is your TF-IDF matrix for the test data and tfidf is your TfidfVectorizer

# Sum the occurrences of each feature across all documents
feature_frequencies = np.asarray(X_test_tfidf_edited.sum(axis=0)).flatten()

# Get the feature names
features = tfidf_edited.get_feature_names_out()

# Create a DataFrame with features and their corresponding frequencies
features_df = pd.DataFrame({'Feature': features, 'Frequency': feature_frequencies})

# Sort the DataFrame by frequency in descending order
features_df = features_df.sort_values(by='Frequency', ascending=False)

# Display the top 20 most frequent features
top_features = features_df.head(50)

# Plot the most frequent features
plt.figure(figsize=(10, 6))
sns.barplot(x='Frequency', y='Feature', data=top_features)
plt.title('Top 20 Most Frequent Features in Test Data')
plt.show()

features_df

from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize CountVectorizer
count_vectorizer = CountVectorizer(max_features=10000, vocabulary=edited_feature_words)  # You can adjust the number of features

# Fit and transform the test data
X_test_counts = count_vectorizer.fit_transform(X_test)

# Sum the occurrences of each feature across all documents
feature_frequencies = np.asarray(X_test_counts.sum(axis=0)).flatten()

# Get the feature names
features = count_vectorizer.get_feature_names_out()

# Create a DataFrame with features and their corresponding frequencies
features_df = pd.DataFrame({'Feature': features, 'Frequency': feature_frequencies})

# Sort the DataFrame by frequency in descending order
features_df = features_df.sort_values(by='Frequency', ascending=False)

# Display the top 20 most frequent features
top_features = features_df.head(20)

# Plot the most frequent features
plt.figure(figsize=(10, 6))
sns.barplot(x='Frequency', y='Feature', data=top_features)
plt.title('Top 20 Most Frequent Features in Test Data')
plt.show()

features_df.head(50)

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Bidirectional, LSTM, BatchNormalization
from tensorflow.keras.models import Model
from transformers import XLMRobertaTokenizer, TFXLMRobertaModel
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import pandas as pd
import re

# Define constants
batch_size = 64
max_length = 60
model_name = 'xlm-roberta-base'

# Data Generator
class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self, data, batch_size, tokenizer, max_length):
        self.data = data
        self.batch_size = batch_size
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.num_samples = len(data)

    def __len__(self):
        return int(np.ceil(self.num_samples / self.batch_size))

    def __getitem__(self, index):
        batch_data = self.data.iloc[index * self.batch_size:(index + 1) * self.batch_size]
        texts = batch_data['text'].tolist()
        labels = batch_data['label'].tolist()

        inputs = self.tokenizer(texts,
                                max_length=self.max_length,
                                padding='max_length',
                                truncation=True,
                                return_tensors='tf')

        return {'input_ids': inputs['input_ids'],
                'attention_mask': inputs['attention_mask']}, np.array(labels)

    def on_epoch_end(self):
        # Optionally shuffle data at the end of each epoch
        pass

# Load and prepare data
df = pd.read_csv('/mnt/HDDa/tayebe/ghadir/stress detection/almas/data_4030501.csv')
df = df[['text', 'label']]
df['text'] = df['text'].fillna('')

def preprocess_text(text):
    text = re.sub(r'@\w+', '', text)  # Remove mentions
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Remove URLs
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)  # Remove non-Persian characters
    return text

df['text'] = df['text'].apply(preprocess_text)

# Train-Test Split
train_data = df
test_data = test_df

# Load XLM-RoBERTa tokenizer and model
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)
xlm_roberta_model = TFXLMRobertaModel.from_pretrained(model_name)

# Instantiate the DataGenerator for training and validation data
train_generator = DataGenerator(train_data, batch_size, tokenizer, max_length)
valid_generator = DataGenerator(test_data, batch_size, tokenizer, max_length)

# Initial training with frozen XLM-RoBERTa layers
xlm_roberta_model.trainable = False

# Model Architecture
input_ids = Input(shape=(max_length,), name='input_ids', dtype=tf.int32)
attention_mask = Input(shape=(max_length,), name='attention_mask', dtype=tf.int32)

xlm_roberta_output = xlm_roberta_model(input_ids, attention_mask=attention_mask)[0]
x = Bidirectional(LSTM(256, return_sequences=True))(xlm_roberta_output)
x = GlobalMaxPooling1D()(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(512, activation="relu")(x)
x = Dense(2, activation='softmax')(x)

model = Model(inputs=[input_ids, attention_mask], outputs=x)

# Compile Model
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8, clipnorm=1.0)
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=optimizer,
    metrics=['accuracy']
)

# Define EarlyStopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',  # Metric to monitor
    patience=3,          # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True,  # Restore model weights from the epoch with the best value of the monitored quantity
    verbose=1            # Verbosity mode
)

# Train with frozen XLM-RoBERTa layers
history = model.fit(
    train_generator,
    epochs=3,
    validation_data=valid_generator,
    callbacks=[early_stopping]
)