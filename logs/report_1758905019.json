{
    "timestamp": "2025-09-26 20:13:39",
    "config": {
        "DATA_PATH_TRAIN": "./data/match_data_4030611.csv",
        "DATA_PATH_TEST": "./data/labeled_data.csv",
        "BERT_MODEL_NAME": "HooshvareLab/bert-fa-base-uncased",
        "MAX_LEN": 64,
        "BATCH_SIZE": 32,
        "BERT_EPOCHS": 1,
        "LEARNING_RATE": 2e-05,
        "ENSEMBLE_WEIGHTS": {
            "bert": 2.0,
            "svm": 1.0,
            "logreg": 1.0,
            "fasttext": 1.0
        },
        "GPU_USAGE_LIMIT": 0.6,
        "SVM_C": 1.0,
        "LOGREG_C": 1.0,
        "FASTTEXT_EPOCHS": 20,
        "FASTTEXT_LR": 0.1,
        "FASTTEXT_WORDNGRAMS": 4
    },
    "individual_model_results": {
        "bert": {
            "accuracy": 0.5137645107794362,
            "precision": 0.6220359069312209,
            "recall": 0.595900373758642,
            "f1_score": 0.5064516518050958
        },
        "svm": {
            "accuracy": 0.5247097844112769,
            "precision": 0.612448467029447,
            "recall": 0.5968837980842359,
            "f1_score": 0.5211487787644948
        },
        "logreg": {
            "accuracy": 0.5067993366500829,
            "precision": 0.6136007496235367,
            "recall": 0.5888391862068042,
            "f1_score": 0.4991750858845683
        },
        "fasttext": {
            "accuracy": 0.5107794361525705,
            "precision": 0.6007799638923501,
            "recall": 0.584801454473098,
            "f1_score": 0.5062110508126414
        }
    },
    "ensemble_results": {
        "accuracy": 0.5124378109452736,
        "precision": 0.6197101727493858,
        "recall": 0.5942542381474888,
        "f1_score": 0.5051858881321871
    }
}