{
    "timestamp": "2025-09-26 21:53:31",
    "config": {
        "DATA_PATH_TRAIN": "./data/match_data_4030611.csv",
        "DATA_PATH_TEST": "./data/labeled_data.csv",
        "BERT_MODEL_NAME": "HooshvareLab/bert-fa-base-uncased",
        "MAX_LEN": 64,
        "BATCH_SIZE": 16,
        "BERT_EPOCHS": 3,
        "LEARNING_RATE": 2e-05,
        "ENSEMBLE_WEIGHTS": {
            "bert": 2.0,
            "svm": 1.0,
            "logreg": 1.0,
            "fasttext": 1.0
        },
        "GPU_USAGE_LIMIT": 0.7,
        "SVM_C": 1.0,
        "LOGREG_C": 1.0,
        "FASTTEXT_EPOCHS": 20,
        "FASTTEXT_LR": 0.1,
        "FASTTEXT_WORDNGRAMS": 4
    },
    "individual_model_results": {
        "bert": {
            "accuracy": 0.46434494195688225,
            "precision": 0.6067592259240262,
            "recall": 0.564111278097667,
            "f1_score": 0.44361258714362256
        },
        "svm": {
            "accuracy": 0.5144278606965174,
            "precision": 0.6307068346542031,
            "recall": 0.5996814872283984,
            "f1_score": 0.505860321321324
        },
        "logreg": {
            "accuracy": 0.5253731343283582,
            "precision": 0.6312246518467877,
            "recall": 0.6057649902853366,
            "f1_score": 0.5192595790910484
        },
        "fasttext": {
            "accuracy": 0.5412935323383085,
            "precision": 0.616798959277157,
            "recall": 0.6065463247385746,
            "f1_score": 0.5396891177067036
        }
    },
    "ensemble_results": {
        "accuracy": 0.5064676616915423,
        "precision": 0.6279526000275204,
        "recall": 0.5942927428450618,
        "f1_score": 0.4962691091942408
    }
}